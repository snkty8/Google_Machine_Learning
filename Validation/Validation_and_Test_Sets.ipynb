{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = \"{:.1f}\".format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv\")\n",
    "test_df = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_factor = 1000.0\n",
    "\n",
    "# Scale the training set's label.\n",
    "train_df[\"median_house_value\"] /= scale_factor \n",
    "\n",
    "# Scale the test set's label\n",
    "test_df[\"median_house_value\"] /= scale_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined the build_model and train_model functions.\n"
     ]
    }
   ],
   "source": [
    "#@title Define the functions that build and train a model\n",
    "def build_model(my_learning_rate):\n",
    "  \"\"\"Create and compile a simple linear regression model.\"\"\"\n",
    "  # Most simple tf.keras models are sequential.\n",
    "  model = tf.keras.models.Sequential()\n",
    "\n",
    "  # Add one linear layer to the model to yield a simple linear regressor.\n",
    "  model.add(tf.keras.layers.Dense(units=1, input_shape=(1,)))\n",
    "\n",
    "  # Compile the model topography into code that TensorFlow can efficiently\n",
    "  # execute. Configure training to minimize the model's mean squared error. \n",
    "  model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=my_learning_rate),\n",
    "                loss=\"mean_squared_error\",\n",
    "                metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "  return model               \n",
    "\n",
    "\n",
    "def train_model(model, df, feature, label, my_epochs, \n",
    "                my_batch_size=None, my_validation_split=0.1):\n",
    "  \"\"\"Feed a dataset into the model in order to train it.\"\"\"\n",
    "\n",
    "  history = model.fit(x=df[feature],\n",
    "                      y=df[label],\n",
    "                      batch_size=my_batch_size,\n",
    "                      epochs=my_epochs,\n",
    "                      validation_split=my_validation_split)\n",
    "\n",
    "  # Gather the model's trained weight and bias.\n",
    "  trained_weight = model.get_weights()[0]\n",
    "  trained_bias = model.get_weights()[1]\n",
    "\n",
    "  # The list of epochs is stored separately from the \n",
    "  # rest of history.\n",
    "  epochs = history.epoch\n",
    "  \n",
    "  # Isolate the root mean squared error for each epoch.\n",
    "  hist = pd.DataFrame(history.history)\n",
    "  rmse = hist[\"root_mean_squared_error\"]\n",
    "\n",
    "  return epochs, rmse, history.history   \n",
    "\n",
    "print(\"Defined the build_model and train_model functions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined the plot_the_loss_curve function.\n"
     ]
    }
   ],
   "source": [
    "#@title Define the plotting function\n",
    "\n",
    "def plot_the_loss_curve(epochs, mae_training, mae_validation):\n",
    "  \"\"\"Plot a curve of loss vs. epoch.\"\"\"\n",
    "\n",
    "  plt.figure()\n",
    "  plt.xlabel(\"Epoch\")\n",
    "  plt.ylabel(\"Root Mean Squared Error\")\n",
    "\n",
    "  plt.plot(epochs[1:], mae_training[1:], label=\"Training Loss\")\n",
    "  plt.plot(epochs[1:], mae_validation[1:], label=\"Validation Loss\")\n",
    "  plt.legend()\n",
    "  \n",
    "  # We're not going to plot the first epoch, since the loss on the first epoch\n",
    "  # is often substantially greater than the loss for other epochs.\n",
    "  merged_mae_lists = mae_training[1:] + mae_validation[1:]\n",
    "  highest_loss = max(merged_mae_lists)\n",
    "  lowest_loss = min(merged_mae_lists)\n",
    "  delta = highest_loss - lowest_loss\n",
    "  print(delta)\n",
    "\n",
    "  top_of_y_axis = highest_loss + (delta * 0.05)\n",
    "  bottom_of_y_axis = lowest_loss - (delta * 0.05)\n",
    "   \n",
    "  plt.ylim([bottom_of_y_axis, top_of_y_axis])\n",
    "  plt.show()  \n",
    "\n",
    "print(\"Defined the plot_the_loss_curve function.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Experiment with the validation split\n",
    "In the following code cell, you'll see a variable named validation_split, which we've initialized at 0.2. The validation_split variable specifies the proportion of the original training set that will serve as the validation set. The original training set contains 17,000 examples. Therefore, a validation_split of 0.2 means that:\n",
    "\n",
    "17,000 * 0.2 ~= 3,400 examples will become the validation set.\n",
    "17,000 * 0.8 ~= 13,600 examples will become the new training set.\n",
    "The following code builds a model, trains it on the training set, and evaluates the built model on both:\n",
    "\n",
    "The training set.\n",
    "And the validation set.\n",
    "If the data in the training set is similar to the data in the validation set, then the two loss curves and the final loss values should be almost identical. However, the loss curves and final loss values are not almost identical. Hmm, that's odd.\n",
    "\n",
    "Experiment with two or three different values of validation_split. Do different values of validation_split fix the problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\snkty\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 40468.7500 - root_mean_squared_error: 201.1685 - val_loss: 49076.6055 - val_root_mean_squared_error: 221.5324\n",
      "Epoch 2/30\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 23363.2109 - root_mean_squared_error: 152.8503 - val_loss: 28758.2754 - val_root_mean_squared_error: 169.5827\n",
      "Epoch 3/30\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 12372.3574 - root_mean_squared_error: 111.2311 - val_loss: 15603.0586 - val_root_mean_squared_error: 124.9122\n",
      "Epoch 4/30\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 7301.4199 - root_mean_squared_error: 85.4483 - val_loss: 9549.3770 - val_root_mean_squared_error: 97.7209\n",
      "Epoch 5/30\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6512.8540 - root_mean_squared_error: 80.7023 - val_loss: 9369.1572 - val_root_mean_squared_error: 96.7944\n",
      "Epoch 6/30\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6513.0669 - root_mean_squared_error: 80.7036 - val_loss: 9346.6143 - val_root_mean_squared_error: 96.6779\n",
      "Epoch 7/30\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6513.6870 - root_mean_squared_error: 80.7074 - val_loss: 9246.4277 - val_root_mean_squared_error: 96.1583\n",
      "Epoch 8/30\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6513.0381 - root_mean_squared_error: 80.7034 - val_loss: 9369.6533 - val_root_mean_squared_error: 96.7970\n",
      "Epoch 9/30\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6512.8970 - root_mean_squared_error: 80.7025 - val_loss: 9374.8887 - val_root_mean_squared_error: 96.8240\n",
      "Epoch 10/30\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6513.1313 - root_mean_squared_error: 80.7040 - val_loss: 9317.1133 - val_root_mean_squared_error: 96.5252\n",
      "Epoch 11/30\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6512.9580 - root_mean_squared_error: 80.7029 - val_loss: 9368.4043 - val_root_mean_squared_error: 96.7905\n",
      "Epoch 12/30\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6513.5083 - root_mean_squared_error: 80.7063 - val_loss: 9157.7988 - val_root_mean_squared_error: 95.6964\n",
      "Epoch 13/30\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6513.0586 - root_mean_squared_error: 80.7035 - val_loss: 9168.3311 - val_root_mean_squared_error: 95.7514\n",
      "Epoch 14/30\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6513.1025 - root_mean_squared_error: 80.7038 - val_loss: 9168.0781 - val_root_mean_squared_error: 95.7501\n",
      "Epoch 15/30\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6514.1045 - root_mean_squared_error: 80.7100 - val_loss: 9295.8721 - val_root_mean_squared_error: 96.4151\n",
      "Epoch 16/30\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6513.7925 - root_mean_squared_error: 80.7081 - val_loss: 9255.1475 - val_root_mean_squared_error: 96.2037\n",
      "Epoch 17/30\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6512.5459 - root_mean_squared_error: 80.7003 - val_loss: 9229.2607 - val_root_mean_squared_error: 96.0690\n",
      "Epoch 18/30\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6513.4316 - root_mean_squared_error: 80.7058 - val_loss: 9262.2607 - val_root_mean_squared_error: 96.2406\n",
      "Epoch 19/30\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6513.3247 - root_mean_squared_error: 80.7052 - val_loss: 9347.9570 - val_root_mean_squared_error: 96.6848\n",
      "Epoch 20/30\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6513.5381 - root_mean_squared_error: 80.7065 - val_loss: 9230.2305 - val_root_mean_squared_error: 96.0741\n",
      "Epoch 21/30\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6513.4570 - root_mean_squared_error: 80.7060 - val_loss: 9264.6650 - val_root_mean_squared_error: 96.2531\n",
      "Epoch 22/30\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6514.0210 - root_mean_squared_error: 80.7095 - val_loss: 9259.7568 - val_root_mean_squared_error: 96.2276\n",
      "Epoch 23/30\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6513.7124 - root_mean_squared_error: 80.7076 - val_loss: 9355.2324 - val_root_mean_squared_error: 96.7224\n",
      "Epoch 24/30\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6513.5366 - root_mean_squared_error: 80.7065 - val_loss: 9182.1543 - val_root_mean_squared_error: 95.8236\n",
      "Epoch 25/30\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6512.5239 - root_mean_squared_error: 80.7002 - val_loss: 9133.5918 - val_root_mean_squared_error: 95.5698\n",
      "Epoch 26/30\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6514.0068 - root_mean_squared_error: 80.7094 - val_loss: 9234.3711 - val_root_mean_squared_error: 96.0956\n",
      "Epoch 27/30\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6512.7783 - root_mean_squared_error: 80.7018 - val_loss: 9325.3906 - val_root_mean_squared_error: 96.5681\n",
      "Epoch 28/30\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6513.4673 - root_mean_squared_error: 80.7061 - val_loss: 9306.8750 - val_root_mean_squared_error: 96.4721\n",
      "Epoch 29/30\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6514.4346 - root_mean_squared_error: 80.7120 - val_loss: 9303.9121 - val_root_mean_squared_error: 96.4568\n",
      "Epoch 30/30\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6512.8008 - root_mean_squared_error: 80.7019 - val_loss: 9210.4404 - val_root_mean_squared_error: 95.9710\n",
      "88.8824462890625\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The following variables are the hyperparameters.\n",
    "learning_rate = 0.08\n",
    "epochs = 30\n",
    "batch_size = 100\n",
    "\n",
    "# Split the original training set into a reduced training set and a\n",
    "# validation set. \n",
    "validation_split = 0.2\n",
    "\n",
    "# Identify the feature and the label.\n",
    "my_feature = \"median_income\"    # the median income on a specific city block.\n",
    "my_label = \"median_house_value\" # the median house value on a specific city block.\n",
    "# That is, you're going to create a model that predicts house value based \n",
    "# solely on the neighborhood's median income.  \n",
    "\n",
    "# Discard any pre-existing version of the model.\n",
    "my_model = None\n",
    "\n",
    "# Invoke the functions to build and train the model.\n",
    "my_model = build_model(learning_rate)\n",
    "epochs, rmse, history = train_model(my_model, train_df, my_feature, \n",
    "                                    my_label, epochs, batch_size, \n",
    "                                    validation_split)\n",
    "\n",
    "plot_the_loss_curve(epochs, history[\"root_mean_squared_error\"], \n",
    "                    history[\"val_root_mean_squared_error\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: Determine why the loss curves differ\n",
    "No matter how you split the training set and the validation set, the loss curves differ significantly. Evidently, the data in the training set isn't similar enough to the data in the validation set. Counterintuitive? Yes, but this problem is actually pretty common in machine learning.\n",
    "\n",
    "Your task is to determine why the loss curves aren't highly similar. As with most issues in machine learning, the problem is rooted in the data itself. To solve this mystery of why the training set and validation set aren't almost identical, write a line or two of pandas code in the following code cell. Here are a couple of hints:\n",
    "\n",
    "The previous code cell split the original training set into:\n",
    "a reduced training set (the original training set - the validation set)\n",
    "the validation set\n",
    "By default, the pandas head method outputs the first 5 rows of the DataFrame. To see more of the training set, specify the n argument to head and assign a large positive integer to n.\n",
    "[ ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-114.3</td>\n",
       "      <td>34.2</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5612.0</td>\n",
       "      <td>1283.0</td>\n",
       "      <td>1015.0</td>\n",
       "      <td>472.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>66.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-114.5</td>\n",
       "      <td>34.4</td>\n",
       "      <td>19.0</td>\n",
       "      <td>7650.0</td>\n",
       "      <td>1901.0</td>\n",
       "      <td>1129.0</td>\n",
       "      <td>463.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>80.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-114.6</td>\n",
       "      <td>33.7</td>\n",
       "      <td>17.0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>333.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>85.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-114.6</td>\n",
       "      <td>33.6</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1501.0</td>\n",
       "      <td>337.0</td>\n",
       "      <td>515.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>73.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-114.6</td>\n",
       "      <td>33.6</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1454.0</td>\n",
       "      <td>326.0</td>\n",
       "      <td>624.0</td>\n",
       "      <td>262.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>65.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>-117.1</td>\n",
       "      <td>32.5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6533.0</td>\n",
       "      <td>1217.0</td>\n",
       "      <td>4797.0</td>\n",
       "      <td>1177.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>144.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>-117.1</td>\n",
       "      <td>34.6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5110.0</td>\n",
       "      <td>1044.0</td>\n",
       "      <td>1938.0</td>\n",
       "      <td>724.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>112.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>-117.1</td>\n",
       "      <td>34.2</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4397.0</td>\n",
       "      <td>931.0</td>\n",
       "      <td>1145.0</td>\n",
       "      <td>445.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>108.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>-117.1</td>\n",
       "      <td>34.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>4144.0</td>\n",
       "      <td>826.0</td>\n",
       "      <td>2127.0</td>\n",
       "      <td>772.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>96.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>-117.1</td>\n",
       "      <td>33.6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1868.0</td>\n",
       "      <td>289.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>247.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>307.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0       -114.3      34.2                15.0       5612.0          1283.0   \n",
       "1       -114.5      34.4                19.0       7650.0          1901.0   \n",
       "2       -114.6      33.7                17.0        720.0           174.0   \n",
       "3       -114.6      33.6                14.0       1501.0           337.0   \n",
       "4       -114.6      33.6                20.0       1454.0           326.0   \n",
       "..         ...       ...                 ...          ...             ...   \n",
       "995     -117.1      32.5                 8.0       6533.0          1217.0   \n",
       "996     -117.1      34.6                 6.0       5110.0          1044.0   \n",
       "997     -117.1      34.2                22.0       4397.0           931.0   \n",
       "998     -117.1      34.0                24.0       4144.0           826.0   \n",
       "999     -117.1      33.6                 6.0       1868.0           289.0   \n",
       "\n",
       "     population  households  median_income  median_house_value  \n",
       "0        1015.0       472.0            1.5                66.9  \n",
       "1        1129.0       463.0            1.8                80.1  \n",
       "2         333.0       117.0            1.7                85.7  \n",
       "3         515.0       226.0            3.2                73.4  \n",
       "4         624.0       262.0            1.9                65.5  \n",
       "..          ...         ...            ...                 ...  \n",
       "995      4797.0      1177.0            4.0               144.4  \n",
       "996      1938.0       724.0            3.2               112.8  \n",
       "997      1145.0       445.0            4.5               108.4  \n",
       "998      2127.0       772.0            2.5                96.0  \n",
       "999       750.0       247.0            4.4               307.6  \n",
       "\n",
       "[1000 rows x 9 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title Double-click for a possible solution to Task 2.\n",
    "\n",
    "# Examine examples 0 through 4 and examples 25 through 29\n",
    "# of the training set\n",
    "train_df.head(n=1000)\n",
    "\n",
    "# The original training set is sorted by longitude. \n",
    "# Apparently, longitude influences the relationship of\n",
    "# total_rooms to median_house_value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3. Fix the problem\n",
    "To fix the problem, shuffle the examples in the training set before splitting the examples into a training set and validation set. To do so, take the following steps:\n",
    "\n",
    "Shuffle the data in the training set by adding the following line anywhere before you call train_model (in the code cell associated with Task 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_train_df = train_df.reindex(np.random.permutation(train_df.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass shuffled_train_df (instead of train_df) as the second argument to train_model (in the code call associated with Task 1) so that the call becomes as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6961.8643 - root_mean_squared_error: 83.4378 - val_loss: 7228.3945 - val_root_mean_squared_error: 85.0200\n",
      "Epoch 2/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6961.8872 - root_mean_squared_error: 83.4379 - val_loss: 7228.6260 - val_root_mean_squared_error: 85.0213\n",
      "Epoch 3/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6960.4834 - root_mean_squared_error: 83.4295 - val_loss: 7233.8159 - val_root_mean_squared_error: 85.0518\n",
      "Epoch 4/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6961.3735 - root_mean_squared_error: 83.4348 - val_loss: 7227.5459 - val_root_mean_squared_error: 85.0150\n",
      "Epoch 5/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6962.0649 - root_mean_squared_error: 83.4390 - val_loss: 7227.5166 - val_root_mean_squared_error: 85.0148\n",
      "Epoch 6/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6961.7192 - root_mean_squared_error: 83.4369 - val_loss: 7227.9805 - val_root_mean_squared_error: 85.0175\n",
      "Epoch 7/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6961.4946 - root_mean_squared_error: 83.4356 - val_loss: 7227.7451 - val_root_mean_squared_error: 85.0161\n",
      "Epoch 8/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6960.8569 - root_mean_squared_error: 83.4317 - val_loss: 7230.8896 - val_root_mean_squared_error: 85.0346\n",
      "Epoch 9/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6961.6851 - root_mean_squared_error: 83.4367 - val_loss: 7228.3989 - val_root_mean_squared_error: 85.0200\n",
      "Epoch 10/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6960.3862 - root_mean_squared_error: 83.4289 - val_loss: 7227.2358 - val_root_mean_squared_error: 85.0132\n",
      "Epoch 11/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6959.6211 - root_mean_squared_error: 83.4243 - val_loss: 7239.9370 - val_root_mean_squared_error: 85.0878\n",
      "Epoch 12/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6961.6016 - root_mean_squared_error: 83.4362 - val_loss: 7233.5977 - val_root_mean_squared_error: 85.0506\n",
      "Epoch 13/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6962.0630 - root_mean_squared_error: 83.4390 - val_loss: 7227.8682 - val_root_mean_squared_error: 85.0169\n",
      "Epoch 14/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6961.4434 - root_mean_squared_error: 83.4353 - val_loss: 7227.0742 - val_root_mean_squared_error: 85.0122\n",
      "Epoch 15/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6961.5405 - root_mean_squared_error: 83.4358 - val_loss: 7228.3267 - val_root_mean_squared_error: 85.0196\n",
      "Epoch 16/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6961.0278 - root_mean_squared_error: 83.4328 - val_loss: 7227.4258 - val_root_mean_squared_error: 85.0143\n",
      "Epoch 17/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6961.9790 - root_mean_squared_error: 83.4385 - val_loss: 7228.7788 - val_root_mean_squared_error: 85.0222\n",
      "Epoch 18/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6961.6543 - root_mean_squared_error: 83.4365 - val_loss: 7227.2764 - val_root_mean_squared_error: 85.0134\n",
      "Epoch 19/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6962.4077 - root_mean_squared_error: 83.4410 - val_loss: 7227.4375 - val_root_mean_squared_error: 85.0143\n",
      "Epoch 20/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6961.2358 - root_mean_squared_error: 83.4340 - val_loss: 7231.5347 - val_root_mean_squared_error: 85.0384\n",
      "Epoch 21/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6961.4214 - root_mean_squared_error: 83.4351 - val_loss: 7228.3755 - val_root_mean_squared_error: 85.0199\n",
      "Epoch 22/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6960.8711 - root_mean_squared_error: 83.4318 - val_loss: 7226.8169 - val_root_mean_squared_error: 85.0107\n",
      "Epoch 23/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6961.4980 - root_mean_squared_error: 83.4356 - val_loss: 7226.8193 - val_root_mean_squared_error: 85.0107\n",
      "Epoch 24/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6961.9800 - root_mean_squared_error: 83.4385 - val_loss: 7228.3364 - val_root_mean_squared_error: 85.0196\n",
      "Epoch 25/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6961.4043 - root_mean_squared_error: 83.4350 - val_loss: 7226.6152 - val_root_mean_squared_error: 85.0095\n",
      "Epoch 26/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6961.2109 - root_mean_squared_error: 83.4339 - val_loss: 7232.1333 - val_root_mean_squared_error: 85.0419\n",
      "Epoch 27/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6961.8770 - root_mean_squared_error: 83.4379 - val_loss: 7228.7754 - val_root_mean_squared_error: 85.0222\n",
      "Epoch 28/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6962.2324 - root_mean_squared_error: 83.4400 - val_loss: 7227.2021 - val_root_mean_squared_error: 85.0130\n",
      "Epoch 29/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6961.2856 - root_mean_squared_error: 83.4343 - val_loss: 7227.0977 - val_root_mean_squared_error: 85.0123\n",
      "Epoch 30/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6961.1685 - root_mean_squared_error: 83.4336 - val_loss: 7232.5425 - val_root_mean_squared_error: 85.0444\n",
      "Epoch 31/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6961.3965 - root_mean_squared_error: 83.4350 - val_loss: 7227.0352 - val_root_mean_squared_error: 85.0120\n",
      "Epoch 32/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6960.5166 - root_mean_squared_error: 83.4297 - val_loss: 7228.8447 - val_root_mean_squared_error: 85.0226\n",
      "Epoch 33/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6961.4966 - root_mean_squared_error: 83.4356 - val_loss: 7230.9194 - val_root_mean_squared_error: 85.0348\n",
      "Epoch 34/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6960.5396 - root_mean_squared_error: 83.4298 - val_loss: 7228.9722 - val_root_mean_squared_error: 85.0234\n",
      "Epoch 35/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6961.1226 - root_mean_squared_error: 83.4333 - val_loss: 7229.9448 - val_root_mean_squared_error: 85.0291\n",
      "Epoch 36/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6961.4302 - root_mean_squared_error: 83.4352 - val_loss: 7228.5088 - val_root_mean_squared_error: 85.0206\n",
      "Epoch 37/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6961.1436 - root_mean_squared_error: 83.4335 - val_loss: 7226.8618 - val_root_mean_squared_error: 85.0109\n",
      "Epoch 38/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6959.8154 - root_mean_squared_error: 83.4255 - val_loss: 7228.5166 - val_root_mean_squared_error: 85.0207\n",
      "Epoch 39/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6961.7437 - root_mean_squared_error: 83.4371 - val_loss: 7228.6470 - val_root_mean_squared_error: 85.0215\n",
      "Epoch 40/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6961.1621 - root_mean_squared_error: 83.4336 - val_loss: 7227.0571 - val_root_mean_squared_error: 85.0121\n",
      "Epoch 41/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6961.2881 - root_mean_squared_error: 83.4343 - val_loss: 7227.2002 - val_root_mean_squared_error: 85.0129\n",
      "Epoch 42/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6961.2012 - root_mean_squared_error: 83.4338 - val_loss: 7226.8486 - val_root_mean_squared_error: 85.0109\n",
      "Epoch 43/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6960.2646 - root_mean_squared_error: 83.4282 - val_loss: 7237.2812 - val_root_mean_squared_error: 85.0722\n",
      "Epoch 44/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6962.6616 - root_mean_squared_error: 83.4426 - val_loss: 7227.2017 - val_root_mean_squared_error: 85.0129\n",
      "Epoch 45/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6961.4751 - root_mean_squared_error: 83.4355 - val_loss: 7228.5996 - val_root_mean_squared_error: 85.0212\n",
      "Epoch 46/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 0s 2ms/step - loss: 6961.6890 - root_mean_squared_error: 83.4367 - val_loss: 7226.7812 - val_root_mean_squared_error: 85.0105\n",
      "Epoch 47/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6960.9575 - root_mean_squared_error: 83.4324 - val_loss: 7228.8970 - val_root_mean_squared_error: 85.0229\n",
      "Epoch 48/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6961.4683 - root_mean_squared_error: 83.4354 - val_loss: 7229.9922 - val_root_mean_squared_error: 85.0294\n",
      "Epoch 49/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6960.6958 - root_mean_squared_error: 83.4308 - val_loss: 7228.3198 - val_root_mean_squared_error: 85.0195\n",
      "Epoch 50/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6961.9116 - root_mean_squared_error: 83.4381 - val_loss: 7230.9146 - val_root_mean_squared_error: 85.0348\n",
      "Epoch 51/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6961.2524 - root_mean_squared_error: 83.4341 - val_loss: 7226.7358 - val_root_mean_squared_error: 85.0102\n",
      "Epoch 52/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6960.3105 - root_mean_squared_error: 83.4285 - val_loss: 7227.3457 - val_root_mean_squared_error: 85.0138\n",
      "Epoch 53/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6962.1660 - root_mean_squared_error: 83.4396 - val_loss: 7226.9033 - val_root_mean_squared_error: 85.0112\n",
      "Epoch 54/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6960.7837 - root_mean_squared_error: 83.4313 - val_loss: 7228.0337 - val_root_mean_squared_error: 85.0178\n",
      "Epoch 55/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6960.0869 - root_mean_squared_error: 83.4271 - val_loss: 7227.2031 - val_root_mean_squared_error: 85.0129\n",
      "Epoch 56/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6960.2388 - root_mean_squared_error: 83.4280 - val_loss: 7227.9722 - val_root_mean_squared_error: 85.0175\n",
      "Epoch 57/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6958.6411 - root_mean_squared_error: 83.4185 - val_loss: 7238.5410 - val_root_mean_squared_error: 85.0796\n",
      "Epoch 58/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6959.7793 - root_mean_squared_error: 83.4253 - val_loss: 7227.3799 - val_root_mean_squared_error: 85.0140\n",
      "Epoch 59/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6961.9004 - root_mean_squared_error: 83.4380 - val_loss: 7227.2017 - val_root_mean_squared_error: 85.0129\n",
      "Epoch 60/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6961.3599 - root_mean_squared_error: 83.4348 - val_loss: 7227.1401 - val_root_mean_squared_error: 85.0126\n",
      "Epoch 61/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6961.2920 - root_mean_squared_error: 83.4344 - val_loss: 7227.0425 - val_root_mean_squared_error: 85.0120\n",
      "Epoch 62/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6960.6704 - root_mean_squared_error: 83.4306 - val_loss: 7232.0518 - val_root_mean_squared_error: 85.0415\n",
      "Epoch 63/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6961.2007 - root_mean_squared_error: 83.4338 - val_loss: 7227.1045 - val_root_mean_squared_error: 85.0124\n",
      "Epoch 64/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6959.8589 - root_mean_squared_error: 83.4258 - val_loss: 7228.7446 - val_root_mean_squared_error: 85.0220\n",
      "Epoch 65/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6961.6499 - root_mean_squared_error: 83.4365 - val_loss: 7231.4336 - val_root_mean_squared_error: 85.0378\n",
      "Epoch 66/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6961.8301 - root_mean_squared_error: 83.4376 - val_loss: 7227.3301 - val_root_mean_squared_error: 85.0137\n",
      "Epoch 67/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6959.5327 - root_mean_squared_error: 83.4238 - val_loss: 7226.8857 - val_root_mean_squared_error: 85.0111\n",
      "Epoch 68/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6961.6689 - root_mean_squared_error: 83.4366 - val_loss: 7226.8574 - val_root_mean_squared_error: 85.0109\n",
      "Epoch 69/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6960.6953 - root_mean_squared_error: 83.4308 - val_loss: 7229.0508 - val_root_mean_squared_error: 85.0238\n",
      "Epoch 70/70\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 6960.3901 - root_mean_squared_error: 83.4290 - val_loss: 7234.8589 - val_root_mean_squared_error: 85.0580\n"
     ]
    }
   ],
   "source": [
    "#@title Double-click to view the complete implementation.\n",
    "\n",
    "# The following variables are the hyperparameters.\n",
    "learning_rate = 0.08\n",
    "epochs = 70\n",
    "batch_size = 100\n",
    "\n",
    "# Split the original training set into a reduced training set and a\n",
    "# validation set. \n",
    "validation_split = 0.2\n",
    "epochs, rmse, history = train_model(my_model, shuffled_train_df, my_feature, \n",
    "                                      my_label, epochs, batch_size, \n",
    "                                      validation_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
